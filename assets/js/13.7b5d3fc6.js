(window.webpackJsonp=window.webpackJsonp||[]).push([[13],{500:function(t,a,s){"use strict";s.r(a);var n=s(21),r=Object(n.a)({},(function(){var t=this,a=t.$createElement,s=t._self._c||a;return s("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[s("h1",{attrs:{id:"torch-autograd简要介绍"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#torch-autograd简要介绍"}},[t._v("#")]),t._v(" torch.autograd简要介绍")]),t._v(" "),s("ul",[s("li",[t._v("torch.autograd是Pytorch的自动查分引擎")]),t._v(" "),s("li",[t._v("可为神经网络提供支持")]),t._v(" "),s("li",[t._v("以下将介绍，Autograd如何帮助神经网络训练的概念性理解")])]),t._v(" "),s("h1",{attrs:{id:"背景"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#背景"}},[t._v("#")]),t._v(" 背景")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("神经网络（NN）是某些输入数据上执行的嵌套函数的集合")])]),t._v(" "),s("li",[s("p",[t._v("这些函数由参数（权重和偏差组成）定义，这些参数在PyTorch中存储在张量中")])]),t._v(" "),s("li",[s("p",[t._v("训练NN分为两个步骤：")]),t._v(" "),s("ol",[s("li",[s("p",[t._v("正向传播")]),t._v(" "),s("p",[t._v("NN对正向的输出进行最佳猜测，它通过其每个函数运行输入数据进行猜测")])]),t._v(" "),s("li",[s("p",[t._v("反向传播")]),t._v(" "),s("p",[t._v("NN根据其猜测中的误差调整其参数，它通过从输出向后遍历，收集有关函数（梯度）的误差导数并使用梯度下降来优化参数来实现。")]),t._v(" "),s("p",[t._v("详细的反向传播演练：")])])])])]),t._v(" "),s("h1",{attrs:{id:"在pytorch中的用法"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#在pytorch中的用法"}},[t._v("#")]),t._v(" 在PyTorch中的用法")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("以下示例，加载了预训练resnet18模型；")])]),t._v(" "),s("li",[s("p",[t._v("创建一个随机数据张量来表示具有3个通道的的单个图像，高&宽为64，其对应的label初始化为一些随机值")]),t._v(" "),s("div",{staticClass:"language-Python、 line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-text"}},[s("code",[t._v("import torch\nimport torchvision\n\nmodel = torchvision.models.resnet18(pretrained=True)\ndata = torch.rand(1, 3, 64, 64)\nlabels = torch.rand(1, 1000)\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br"),s("span",{staticClass:"line-number"},[t._v("5")]),s("br"),s("span",{staticClass:"line-number"},[t._v("6")]),s("br")])])]),t._v(" "),s("li",[s("p",[t._v("正向传播：通过模型的每一层运行输入数据进行预测")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 正向传播")]),t._v("\nprediction "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("data"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br")])])]),t._v(" "),s("li",[s("p",[t._v("我们使用模型的预测和相应的标签来计算误差（loss）")])]),t._v(" "),s("li",[s("p",[t._v("接着通过网络反向传播此误差")])]),t._v(" "),s("li",[s("p",[t._v("在误差张量上调用.backward()时，开始方向传播")])]),t._v(" "),s("li",[s("p",[t._v("然后，autograd会为每个模型参数计算梯度，并将其存储在参数的.grad属性中")]),t._v(" "),s("div",{staticClass:"language-python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 反向传播")]),t._v("\nloss "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("prediction "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("-")]),t._v(" labels"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),s("span",{pre:!0,attrs:{class:"token builtin"}},[t._v("sum")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nloss"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("backward"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# backward pass")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br")])])]),t._v(" "),s("li",[s("p",[t._v("加载一个优化器，在本例中为SGD，学习率为0.01，动量为0.9，我们在优化器中注册模型的所有参数")]),t._v(" "),s("div",{staticClass:"language-Python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("optim "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("optim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("SGD"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),t._v("model"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("parameters"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" lr"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("1e-2")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" momentum"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("0.9")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n\n"),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# 调用.step()启动梯度下降，优化器通过.grad中存储的梯度来调整每个参数")]),t._v("\noptim"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("step"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("  "),s("span",{pre:!0,attrs:{class:"token comment"}},[t._v("# gradient descent")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br"),s("span",{staticClass:"line-number"},[t._v("3")]),s("br"),s("span",{staticClass:"line-number"},[t._v("4")]),s("br")])])])]),t._v(" "),s("h1",{attrs:{id:"autograd微分"}},[s("a",{staticClass:"header-anchor",attrs:{href:"#autograd微分"}},[t._v("#")]),t._v(" Autograd微分")]),t._v(" "),s("ul",[s("li",[s("p",[t._v("autograd是如何收集梯度的；")])]),t._v(" "),s("li",[s("p",[t._v("用required_grad = True 创建两个张量a，b。")])]),t._v(" "),s("li",[s("p",[t._v("向autograd发出信号，应跟踪对他们的操作")]),t._v(" "),s("div",{staticClass:"language-Python line-numbers-mode"},[s("pre",{pre:!0,attrs:{class:"language-python"}},[s("code",[t._v("a "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("2.")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("3.")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" requires_grad"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\nb "),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),t._v(" torch"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(".")]),t._v("tensor"),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("(")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("[")]),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("6")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" "),s("span",{pre:!0,attrs:{class:"token number"}},[t._v("4")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v("]")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(",")]),t._v(" requires_grad"),s("span",{pre:!0,attrs:{class:"token operator"}},[t._v("=")]),s("span",{pre:!0,attrs:{class:"token boolean"}},[t._v("True")]),s("span",{pre:!0,attrs:{class:"token punctuation"}},[t._v(")")]),t._v("\n")])]),t._v(" "),s("div",{staticClass:"line-numbers-wrapper"},[s("span",{staticClass:"line-number"},[t._v("1")]),s("br"),s("span",{staticClass:"line-number"},[t._v("2")]),s("br")])])]),t._v(" "),s("li",[s("p",[t._v("我们从a和b创建另一个张量Q。")])]),t._v(" "),s("li",[s("p",[t._v("假设a、b是神经网络参数，Q是误差。在NN训练中，我们想要对于参数的误差即当在Q上")])]),t._v(" "),s("li",[s("p",[t._v("调用.backward()时，autograd将计算这些梯度并将其存储在各个张量的grad属性中")])]),t._v(" "),s("li",[s("p",[t._v("我们需要在"),s("code",[t._v("Q.backward()")]),t._v("中"),s("strong",[t._v("显式传递")]),s("code",[t._v("gradient")]),t._v("参数，因为它是向量。 "),s("code",[t._v("gradient")]),t._v("是与"),s("code",[t._v("Q")]),t._v("形状相同的张量，它表示"),s("code",[t._v("Q")]),t._v("相对于本身的梯度，即")])]),t._v(" "),s("li",[s("p",[t._v("同样，我们也可以将"),s("code",[t._v("Q")]),t._v("聚合为一个标量，然后隐式地向后调用，例如"),s("code",[t._v("Q.sum().backward()")]),t._v("。")])])])])}),[],!1,null,null,null);a.default=r.exports}}]);