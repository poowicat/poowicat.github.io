(window.webpackJsonp=window.webpackJsonp||[]).push([[70],{560:function(t,a,_){"use strict";_.r(a);var s=_(21),v=Object(s.a)({},(function(){var t=this,a=t.$createElement,_=t._self._c||a;return _("ContentSlotsDistributor",{attrs:{"slot-key":t.$parent.slotKey}},[_("h3",{attrs:{id:"batch-size的作用"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#batch-size的作用"}},[t._v("#")]),t._v(" Batch_size的作用：")]),t._v(" "),_("ol",[_("li",[t._v("决定了下降的方向。")])]),t._v(" "),_("h3",{attrs:{id:"在合理范围内增大batch-size的好处"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#在合理范围内增大batch-size的好处"}},[t._v("#")]),t._v(" 在合理范围内增大Batch_size的好处：")]),t._v(" "),_("ul",[_("li",[_("strong",[t._v("提高内存利用率")]),t._v("以及大矩阵乘法的并行化效率；")]),t._v(" "),_("li",[t._v("跑完一次epoch（全数据集）所需要的"),_("strong",[t._v("迭代次数减少")]),t._v("，对相同的数据量，"),_("u",[t._v("处理速度比小的Batch_size要更快")]),t._v("；")]),t._v(" "),_("li",[t._v("在一定范围内，一般来说Batch_size越大，其确定的下降方向越准，引起的"),_("u",[t._v("训练震荡越小")])])]),t._v(" "),_("h3",{attrs:{id:"盲目增大batch-size-batch-size过大的坏处"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#盲目增大batch-size-batch-size过大的坏处"}},[t._v("#")]),t._v(" 盲目增大Batch_size，Batch_size过大的坏处：")]),t._v(" "),_("ul",[_("li",[t._v("提高了内存利用率，但"),_("strong",[t._v("内存容量可能撑不住")]),t._v("；")]),t._v(" "),_("li",[t._v("跑完一次epoch所需的迭代次数变小，但想要达到相同的精度，"),_("s",[t._v("其所花费的时间大大增加")]),t._v("，从而对参数的修正也显得更加缓慢。")]),t._v(" "),_("li",[t._v("Batch_size增大到一定程度，其确定的下降方向已经基本不再变化（"),_("s",[t._v("会影响随机性的引入")]),t._v("）")])]),t._v(" "),_("h3",{attrs:{id:"batch-size的两种极端设置"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#batch-size的两种极端设置"}},[t._v("#")]),t._v(" Batch_size的两种极端设置")]),t._v(" "),_("h4",{attrs:{id:"一、batch-size为全数据集-full-batch-learning"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#一、batch-size为全数据集-full-batch-learning"}},[t._v("#")]),t._v(" 一、batch_size为全数据集(Full Batch Learning)：")]),t._v(" "),_("p",[t._v("在"),_("strong",[t._v("数据集比较小")]),t._v("时使用，"),_("strong",[t._v("好处是")]),t._v("：")]),t._v(" "),_("p",[t._v("由全数据集确定的方向能够更好地代表样本总体，从而更准确地朝向极值所在的方向；\n由于不同权重的梯度值差别巨大，因此选择一个全局的学习率很困难。Full Batch Learning可以使用Rprop只基于梯度符号并且针对性单独更新各权值。")]),t._v(" "),_("p",[t._v("在更"),_("strong",[t._v("大的数据集")]),t._v("上使用的话，好处会变成"),_("strong",[t._v("坏处")]),t._v("：")]),t._v(" "),_("p",[t._v("随着数据集的海量增长和内存限制，一次性载入所有的数据进来变得越来越不可行；\n以Rprop的方式迭代，会由于各个Batch之间的采样差异性，各次梯度修正值相互抵消，无法修正。")]),t._v(" "),_("h4",{attrs:{id:"二、将batch-size设置为1"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#二、将batch-size设置为1"}},[t._v("#")]),t._v(" 二、将Batch_size设置为1：")]),t._v(" "),_("p",[t._v("Batch_size=1，也就是每次只训练一个样本。这就是"),_("u",[t._v("在线学习")]),t._v("(Online Learning)。理论上说batch_size=1是最好的，不过实际上调的时候，会出现batch_size太小导致"),_("u",[t._v("网络收敛不稳定")]),t._v("，"),_("u",[t._v("最后结果比较差")]),t._v("。")]),t._v(" "),_("p",[t._v("这是因为线性神经元在均方"),_("u",[t._v("误差代价函数的错误面是一个抛物面")]),t._v("，横截面是椭圆。对于多层神经元，非线性网络，在局部依然近似是抛物面。使用在线学习，每次修正方向以各自样本的梯度方向修正，难以达到收敛。")]),t._v(" "),_("h3",{attrs:{id:"一些经验之谈"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#一些经验之谈"}},[t._v("#")]),t._v(" 一些经验之谈：")]),t._v(" "),_("p",[t._v("一般而言，根据GPU显存，"),_("u",[t._v("设置为最大")]),t._v("，而且一般要求是８的倍数（比如16，32，64），GPU内部的并行计算效率最高。\n或者选择一部分数据，设置几个８的倍数的Batch_Size，看看loss的下降情况，再选用效果更好的值。\n"),_("img",{attrs:{src:"https://cdn.jsdelivr.net/gh/poowicat/pic_store@main/blog/image.4s8e5fgdgc20.webp",alt:"image"}})]),t._v(" "),_("h3",{attrs:{id:"总结"}},[_("a",{staticClass:"header-anchor",attrs:{href:"#总结"}},[t._v("#")]),t._v(" 总结：")]),t._v(" "),_("p",[t._v("batch_size设的大一些，收敛得快，也就是需要训练的次数少，准确率上升的也很稳定，"),_("u",[t._v("但是实际使用起来精度不高")]),t._v("；\nbatch_size设的小一些，收敛得慢，可能准确率来回震荡，因此"),_("u",[t._v("需要把基础学习速率降低一些")]),t._v("，但是实际使用起来精度较高。")])])}),[],!1,null,null,null);a.default=v.exports}}]);