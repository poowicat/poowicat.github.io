#### 为什么需要对数值类型的特征做归一化？

##### 解答：

对数值归一化，可以将所有的特征都统一到一个**大致相同的数值区间内**，最常用的方法主要有以下**两种：**

1. **线性**函数的归一化

   - 对原始数据进行线性变换。使结果映射到【0,1】范围

   - 实现对原始数据的等比缩放

   - 归一化公式如下：

     ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.5ugfe3r7odw0.webp)

   - 其中X为**原始数据**，Xmax、Xmin分别为数据最大值和最小值。

2. **零均值**归一化

   - 将原始数据映射到**均值**为0、**标准差**为1的分布上

   - 假设原始特征的**均值**为μ、**标准差**为σ（sigma），那么 归一化公式定义为：

     ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.58br6uvy75k0.webp)

3. 为什么需要对数值特征做归一化呢？

   - 借助**梯度下降**来说明归一化重要性

   - **假设：**两种数值型特征，**x1**的取值范围为 [0, 10]，**x2**的取值 范围为[0, 3]，于是可以构造一个目标函数符合图1.1（a）中的等值图。

     ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.77cgz84fuw00.webp)

   - **分析：**

     - 由上图可看，在学习速率相同的情况下，x1的更新速度会大于x2，所以需要更多的迭代才能找到最优解；
     - 如果，将x1，x2归一化到相同的区间，优化目标的等值图，会变成图1.1（b）中的圆形；
     - x1,x2的更新速度变为更一致，容易更快速地通过梯度下降找到最优解。

   - **总结：**

     - 归一化并不是万能的
     - 实际应用中，只支持通过梯度下降法求解的模型（才需要归一化），包括 线性回归、逻辑回归、支持向量机、神经网络等模型
     - 对于决策树模型不适用。



#### 怎样处理类别型特征？ 

##### 解答：

1. 类别型特征

   - 主要指性别、血型等（有限选项内取值的特征）
   - 输入：字符串型
   - 支持：决策树等少数模型。
   - 不支持：逻辑回归、支持向量机。必须处理转换成数值型才能正确工作

2. 序号编码（Ordinal Encoding）、独热编码（One-hot Encoding）、二进制编码 （Binary Encoding）

   - 序号编码：例如成绩（可分为中，高，低档）

   - 独热编码：处理类别间不具有大小关系的特征，例如血型

     - 需注意以下问题：
       1. 使用稀疏向量来节省空间
       2. 配合特征选择来降低维度：
          - K邻近算法中，高维会使两点间很难得到有效的衡量
          - 逻辑回归中模型中：参数的数量会随着维度的增高而增加。容易引起 **过拟合问题**
          - 通常只有部分维度是对分
          - 类、预测有帮助，因此可以考虑配合 **特征选择来降低维度**

   - 二进制编码：

     1. 先用序号编码给每个类，赋予一个类别ID
     2. 对类别ID对应的二进制编码作为结果
     3. 可以看出，二进制编码本 质上是利用二进制对ID进行哈希映射，最终得到0/1特征向量

     优点：

     - 且维数少于独热编 码，**节省了存储空间**。

#### 什么是组合特征？





#### 如何处理高维组合特征？





#### 怎样有效地找到组合特征？





#### 有哪些文本表示模型？它们各有什么优缺点？ 





#### 如何缓解图像分类任务中训练数据不足带来的问题？





#### Word2Vec是如何工作的？它和隐狄利克雷模型有什么区别与联系？ 

