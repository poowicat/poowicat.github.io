---
title: torch.autograd的简要介绍
date: 2022-05-24 14:02:37
permalink: /pages/b330ca/
categories:
  - 深度学习
  - 算法网络
  - PyTorch笔记
tags:
  - 
author: 
  name: poowicat
  link: https://github.com/poowicat
---
# torch.autograd简要介绍

- torch.autograd是Pytorch的自动查分引擎
- 可为神经网络提供支持
- 以下将介绍，Autograd如何帮助神经网络训练的概念性理解

# 背景

- 神经网络（NN）是某些输入数据上执行的嵌套函数的集合

- 这些函数由参数（权重和偏差组成）定义，这些参数在PyTorch中存储在张量中

- 训练NN分为两个步骤：

  1. 正向传播

     NN对正向的输出进行最佳猜测，它通过其每个函数运行输入数据进行猜测

  2. 反向传播

     NN根据其猜测中的误差调整其参数，它通过从输出向后遍历，收集有关函数（梯度）的误差导数并使用梯度下降来优化参数来实现。

     详细的反向传播演练：

     [3Blue1Brown 的视频]: https://www.youtube.com/watch?v=tIeHLnjs5U8

     

# 在PyTorch中的用法

- 以下示例，加载了预训练resnet18模型；

- 创建一个随机数据张量来表示具有3个通道的的单个图像，高&宽为64，其对应的label初始化为一些随机值

  ```Python、
  import torch
  import torchvision
  
  model = torchvision.models.resnet18(pretrained=True)
  data = torch.rand(1, 3, 64, 64)
  labels = torch.rand(1, 1000)
  ```

- 正向传播：通过模型的每一层运行输入数据进行预测

  ```python
  # 正向传播
  prediction = model(data)
  ```

- 我们使用模型的预测和相应的标签来计算误差（loss）

- 接着通过网络反向传播此误差

- 在误差张量上调用.backward()时，开始方向传播

- 然后，autograd会为每个模型参数计算梯度，并将其存储在参数的.grad属性中

  ```python
  # 反向传播
  loss = (prediction - labels).sum()
  loss.backward()  # backward pass
  ```

- 加载一个优化器，在本例中为SGD，学习率为0.01，动量为0.9，我们在优化器中注册模型的所有参数

  ```Python
  optim = torch.optim.SGD(model.parameters(), lr=1e-2, momentum=0.9)
  
  # 调用.step()启动梯度下降，优化器通过.grad中存储的梯度来调整每个参数
  optim.step()  # gradient descent
  ```

  

# Autograd微分

- autograd是如何收集梯度的；

- 用required_grad = True 创建两个张量a，b。

- 向autograd发出信号，应跟踪对他们的操作

  ```Python
  a = torch.tensor([2., 3.], requires_grad=True)
  b = torch.tensor([6, 4], requires_grad=True)
  ```

- 我们从a和b创建另一个张量Q。

-  假设a、b是神经网络参数，Q是误差。在NN训练中，我们想要对于参数的误差即当在Q上

-  调用.backward()时，autograd将计算这些梯度并将其存储在各个张量的grad属性中

- 我们需要在`Q.backward()`中**显式传递**`gradient`参数，因为它是向量。 `gradient`是与`Q`形状相同的张量，它表示`Q`相对于本身的梯度，即

- 同样，我们也可以将`Q`聚合为一个标量，然后隐式地向后调用，例如`Q.sum().backward()`。

