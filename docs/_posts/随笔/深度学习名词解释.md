---
title: 深度学习名词解释
date: 2022-05-24 14:06:44
permalink: /pages/2ecdb1/
sidebar: auto
categories:
  - 随笔
tags:
  - 
author: 
  name: poowicat
  link: https://github.com/poowicat
---


### 梯度

梯度的本意是一个向量（矢量：在[数学](https://baike.baidu.com/item/数学/107037)上定义为一系列由线连接的点），表示某一函数在该点处的方向导数沿着该方向取得最大值，即函数在该点处沿着该方向（此梯度的方向）变化最快，变化率最大（为该梯度的模）。



### 卷积核

卷积核就是图像处理时，给定输入图像，输入图像中**一个小区域**中像素[加权平均](https://baike.baidu.com/item/加权平均/9702101)后成为输出图像中的每个对应像素，其中权值由一个函数定义，**这个函数称为卷积核**。

本质就是**比较**图像邻近像素的相似性。



### 传播

#### 前向传播

1. 输入层-->隐含层
2. 计算各个神经元的输入加权和
3. 神经元的输出（此处用激活函数）
4. 同理推出各个神经元的输出
5. 隐含层-->输出层
6. 计算各个神经元的输出值
7. 与实际 **目标值** 还差很远，所以我们需要对误差进行**反向传播**，更新权值，重新计算输出。

#### 反向传播

1. 计算误差

   总误差

   如两个输出，分别计算各个输出的误差，总误差为**两者之和**。（多个以此类推）

2. 隐含层-->输出层的权值更新

   用总误差对某个需要更新的权重（如叫w5）求偏导数（链式法则）：

   ![image](../../%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.5sn8spc8zic0.webp)

   下面的图可以更直观的看清楚误差是怎样反向传播的：

   

   ![image](../../%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.xizkhvccb74.webp)

3..隐含层---->隐含层的权值更新

![image](../../%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.6saixcse87g0.webp)

[参考链接]: https://blog.csdn.net/weixin_38347387/article/details/82936585



### Sigmoid函数

sigmoid函数也叫[Logistic函数](https://baike.baidu.com/item/Logistic函数/3520384)，用于隐层神经元输出，取值范围为(0,1)，它可以将一个实数映射到(0,1)的区间，可以用来做二分类。在特征相差比较复杂或是相差不是特别大时效果比较好。Sigmoid作为激活函数有以下优缺点：

- 优点：平滑、易于求导。

- 缺点：激活函数计算量大，反向传播求**误差梯度**时，求导涉及除法；反向传播时，很容易就会出现梯度消失的情况，从而无法完成深层网络的训练。

- 定义：

  ![img](../../%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/7e627f6301407c3d610d2c2cab711f3f.svg)

- 对x的导数可以用自身表示：

  ![img](../../%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/06abbe5c050770300de9c91bc76e4af5.svg)

- Sigmoid函数的图形如S曲线

  ![Sigmoid 曲线](../../%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/format,f_auto.png)

### 梯度消失

在神经网络中，当前面隐藏层的学习速率低于后面隐藏层的学习速率，即随着隐藏层数目的增加，**分类准确率**反而**下降了**。、、、

#### 解决方案

- 预训练加微调
  - 局部寻找最优，然后整合寻找全局最优
- 梯度剪切、正则
  - 剪切：梯度爆炸，设置阈值
  - 正则：
- LSTM 长短期记忆网络（long-short term memory networks）经常用于生成文本

### In-place

#### 1、前言

就地操作（In-place)：具有_后缀的操作就是就地操作。例如x.copy _(y), x.t _(), 将改变x。

#### 2、就地操作（In-place)

- pytoch 中原地操作后缀为_， 如`.add()`或`.scatter()`, 就地操作是直接更改给定Tenser的内容而不进行复制的操作。即不会为变量分配新的内存，Python操作类似+=或*=也就是就地操作

- 举例子说明in-place为什么可以在处理高维数据时，帮助减少内存，定义以下简单函数来测量Pytorch的异位ReLU（out-of-place）和就地ReLU（in-place）分配的内存：

- ```python
  import torch # import main library
  import torch.nn as nn # import modules like nn.ReLU()
  import torch.nn.functional as F # import torch functions like F.relu() and F.relu_()
  
  def get_memory_allocated(device, inplace = False):
      '''
      Function measures allocated memory before and after the ReLU function call.
      INPUT:
        - device: gpu device to run the operation
        - inplace: True - to run ReLU in-place, False - for normal ReLU call
      '''
      
      # Create a large tensor
      t = torch.randn(10000, 10000, device=device)
      
      # Measure allocated memory
      torch.cuda.synchronize()
      start_max_memory = torch.cuda.max_memory_allocated() / 1024**2
      start_memory = torch.cuda.memory_allocated() / 1024**2
      
      # Call in-place or normal ReLU
      if inplace:
          F.relu_(t)
      else:
          output = F.relu(t)
      
      # Measure allocated memory after the call
      torch.cuda.synchronize()
      end_max_memory = torch.cuda.max_memory_allocated() / 1024**2
      end_memory = torch.cuda.memory_allocated() / 1024**2
      
      # Return amount of memory allocated for ReLU call
      return end_memory - start_memory, end_max_memory - start_max_memory
      # setup the device
  device = torch.device('cuda:0' if torch.cuda.is_available() else "cpu")
  #开始测试
  # Call the function to measure the allocated memory for the out-of-place ReLU
  memory_allocated, max_memory_allocated = get_memory_allocated(device, inplace = False)
  print('Allocated memory: {}'.format(memory_allocated))
  print('Allocated max memory: {}'.format(max_memory_allocated))
  #Then call the in-place ReLU as follows:
  memory_allocated_inplace, max_memory_allocated_inplace = get_memory_allocated(device, inplace = True)
  print('Allocated memory: {}'.format(memory_allocated_inplace))
  print('Allocated max memory: {}'.format(max_memory_allocated_inplace))
  ```

- 输出结果：

- ```python3
  Allocated memory: 382.0
  Allocated max memory: 382.0
  Allocated memory: 0.0
  Allocated max memory: 0.0
  ```

- 优点：以上看来就地操作可以帮助我们节省一点GPU内存，但是使用需谨慎。

- 缺点：

  1. 可能会覆盖计算 **梯度**所需的值， 这意味着破坏模型的训练过程
  2. 每个in-place实际上都需实现来重写计算图，异地操作Out-of-place分配新对象并保留对旧图的引用，而in-place操作则需要更改表示此操作的函数的所有输入的创建者。

- 官方文档：如果你使用了in-place operation而没

  有报错的话，那么你可以确定你的梯度计算是正确的。另外**尽量避免in-place的使用。**

  ******Autograd**中支持就地操作很困难，并且在大多数情况下不鼓励使用。Autograd积极的缓冲区释放和重用使其非常高效，就地操作实际上降低内存使用量的情况很少。除非在沉重的内存压力下运行，否则可能永远不需要使用它们。



#### 总结

- Autograd已经很够用了。就地操作需要慎用！



[参考文章]: https://zhuanlan.zhihu.com/p/344455805



