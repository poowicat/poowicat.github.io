---
title: 卷积核类型简介
date: 2022-05-17 10:12:47
permalink: /pages/8c63e3/
sidebar: auto
titleTag: 原创
categories:
  - 随笔
tags:
  - 
author: 
  name: poowicat
  link: https://github.com/poowicat
---
### 简介：

![卷积核](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/%E5%8D%B7%E7%A7%AF%E6%A0%B8.yeoypfv5neo-165277311790624.gif)

- 卷积使用kernel从输入图像提取特征
- kernel是矩阵
- 可在图像时滑动并输入相乘
- 以某种我们期望的方式 **增强**输出
- 上面的kernel可用于锐化图像。
- 如下图：中心值为3 * 5 + 2 * -1 + 2 * -1 + 2 * -1 + 2 * -1 =7，值3增加到7。第二个图像，输出是1 * 5 + 2 * -1 + 2 * -1 + 2 * -1 + 2 * -1 = -3，值1减少到-3。显然，3和1之间的对比度增加到了7和-3，图像将更清晰锐利：
- ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.4da6sqanyjs0-165277315021628.webp)
- 通过深层CNN，我们无需再手工设计kernel来提取特征，而是可以直接学习这些可提取潜在特征的kernel值



### Kernel与Fliter

1. 区分kernel和Fliter
   - kernel是权重 矩阵，将权重矩阵与输入相乘提取相关特征，卷积名称就是kernel矩阵的 **维数**
   - filter与kernel串联，每个kernel分配给输入的特定通道，
   - filter总比kernel大一维
   - 例如：在2D卷积中，filter是3D矩阵（本质上是2D（即kernel）的串联）
   - 具有kernel尺寸h*w和输入通道k的cnn层，filter尺寸为K * H * w
   - 普通的卷积层实际上由多个这样的filter组成。*为了简化下面的讨论，除非另有说明，否则假定仅存在一个filter，因为所有filter都会重复相同的操作。*



### 1D,2D,3D卷积

- 一维卷积通常用于时间序列数据分析；
- 一维数据输入可具有多个通道
- 滤波器只能沿一个方向移动，因此输出为1D
- ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.7cgsnwzqwuo0-165277319168930.webp)
- 在下图中，kernel尺寸为3 * 3，并且filter中有多个这样的kernel（标记为黄色）。这是因为输入中有多个通道（标记为蓝色），并且我们有一个kernel对应于输入中的每个通道。显然，这里的filter可以在两个方向上移动，因此最终输出是2D。2D卷积是最常见的卷积，在计算机视觉中大量使用。
- ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.13qw85n9j128.webp)
- 很难可视化3D filter（因为它是4D矩阵)，因此我们在这里只讨论单通道3D卷积。如下图所示，在3D卷积中，kernel可以在3个方向上移动，因此获得的输出也是3D。
- ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.1v8y85yhijhc-165277320766033.webp)

## **转置(Transposed)卷积**

- 下图gif展示2D卷积如何减小输入尺寸
- 但有时我们需要对输入进行上采样等处理
- ![2](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/2.6y9dgyeb4e00-165277321587235.gif)
- 为了使卷积实现此目标，使用转置卷积或反卷积（他并不是真正的逆转卷积运算）的卷积修改版，下面gif的虚线框表示padding
- ![3](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/3.5wbz23uchgg0-165277322511937.gif)
- 这些动画很直观的展示了如何基于不同的padding模式从同一输入产生不同的**上采样**输出。这种卷积在现代CNN网络中非常常用，主要是因为它们具有**增加图像尺寸的能力。**
- ![4](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/4.6inm64wlsl4-165277323320039.webp)

#### 可分离卷积

- 指卷积kernel分解为低维度、kernel

- 可分离卷积有两种主要类型，

- 1、空间上可分离卷积

- > ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.327iw7tiigc0-165277323995541.webp)

- 2、深度可分离卷积（广泛应用于轻量级CNN模型中。并提供了非常好的性能，

- 具有2个输入通道的128个filter的标准2D卷积

- ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.3k5clxuizlo0-165277324757243.webp)

- 深度可分离的2D卷积，它首先分别处理每个通道，然后进行通道间卷积

- ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.3r6ehyuwnv00-165277325378945.webp)

- 用可分离卷积原因：**效率！！！** 

  - 可显著减少所需参数量

### 扩张/空洞（Dilated/Atrous）卷积
- 所有卷积层，他们都是一起处理所有相邻值
- 但是有时跳过某些输入值可能会更好
- 这就是为什么引入扩张卷积的原因（也称空洞卷积）
- 它的修改允许kernel在不增加参数数量下增加其**感受野**
- ![image](https://cdn.jsdelivr.net/gh/poowicat/pic_store@main/blog/5.4cv0ww2bwxi0.gif)
- 上图可视，kernel可以使用与之前相同的9个参数，来构造更广阔的领域
- 但会导致信息丢失
- 但是某些应用中，总体效果是正面的



### 可变形卷积

- kernel形状仅为正方形/矩形（或其他一些需要手动确定的形状）

- 因此它们只能在这种模式下使用，如果卷积形状本身可学习呢？

- 这是引入可变形卷积背后的思想

- ![image](C:/Users/ziyih/Documents/%E7%AC%94%E8%AE%B0/%E5%9B%BE%E7%89%87/image.3dff9ro5wzg0-165277328259948.webp)

- 实现：

  - 每个kernel都用两个不同的矩阵表示
  - 第一分支，从原点 **预测** 偏移
  - 此偏移量表示 要处理原点周围哪些输入
  - 由于每个偏移量都是独立预测的
  - 它们之间无需形成任何刚需形状
  - 第二分支是卷积分支，其输入是这些偏移量处的值

  

  ### 总结

  1. CNN层有多种变体。可单独使用也可结合使用。已创建成功且复杂的体系结构，每个变化都是基于特征提取应如何工作的直觉产生的
  2. 因此，尽管这些深层CNN网络学到了我们无法解释的权重。但是相信它们的直觉对于它们的性能非常重要，朝着这个方向进行进一步的研究工作对于高度复杂的CNN的成功至关重要。
  
  ​		
  
  
  
  

